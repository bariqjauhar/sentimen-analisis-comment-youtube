---
title: "data cleaning"
author: "Bariq Jauhar"
date: "1/17/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r rlib}
library(tm) #memberssihkan data
library(vroom) #membersihkan dataset
library(here) #menyimpan dataset
```

```{r load dataset}
d <- vroom(here('comment_joebidden.csv'))
ulasan <- d$textOriginal
ulasan1 <- Corpus(VectorSource(ulasan))
removeURL <- function(x) gsub("http[^[:space:]]*","",x)
reviewclean <- tm_map(ulasan1,removeURL)
removeNL <- function(y) gsub("\n", "", y)
reviewclean <- tm_map(reviewclean, removeNL)
replacecomma <-function(y) gsub(",", "", y)
reviewclean <- tm_map(reviewclean, replacecomma)
removetitik2 <- function(y) gsub(":", "", y)
reviewclean <- tm_map(reviewclean, removetitik2)
removetitikkoma <- function(y) gsub(";", "", y)
reviewclean <- tm_map(reviewclean, removetitikkoma)
removetitik3 <- function(y) gsub("pâ€¦", "", y)
reviewclean <- tm_map(reviewclean, removetitik3)
removeamp <- function(y) gsub("&amp;", "", y)
reviewclean <- tm_map(reviewclean, removeamp)
removeUN <- function(z) gsub("@\\W+", "", z)
reviewclean <- tm_map(reviewclean, removeUN)
remove.all <- function(xy) sub("[^[:alpha:][:space:]]*", "", xy)
reviewclean <- tm_map(reviewclean, remove.all)
reviewclean <- tm_map(reviewclean, removePunctuation)
reviewclean <- tm_map(reviewclean, tolower)
myStopwords = readLines("stopwords-en.txt")
reviewclean <- tm_map(reviewclean,removeWords,myStopwords)

dataframe<-data.frame(text=unlist(sapply(reviewclean, '[')),stringsAsFactors = F)
View(dataframe)
write.csv(dataframe,file = 'ulasanclean.csv')















```






